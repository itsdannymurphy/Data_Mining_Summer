{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Clustering and Ensemble Learning\n",
    "### Danny Murphy, Kerry Nakayama, Brandon Tarr\n",
    "## Introduction\n",
    "In lab 1 we tackled the rather large dataset provided by Expedia through Kaggle and determined that for the scope of our assignments and learning objectives this semester we would reduce our focus to records that fully transacted as \"bookings\" (~10% of original dataset size) and the top five classes that we've been tasked with classifying (~10% of bookings dataset size). We then used the roughly 410,000 records to explore logistic regression and support vector machines consistently acheiving around 36% accuracy in the Mini Lab. In Lab 2 we further explored the AdaBoostClassifier, ExtraTreesClassifier,  RandomForestClassifier, and GradientBoostingClassifier algorithms concluding that Random Forest increasing classification accuracy in the most computationally efficient manner. Gradient Boosted Trees by far performed the best but at a considerably higher computational expense. We also did a significant amount of investigation of feature importances that included experimentation with a variety of different data subsets finding that a small group of 2-3 features tend to produce smaller, more efficient, and more accurate classification models than the full dataset. \n",
    "Following in the same vein, we're interested in clustering as an alternative preprocessing strategy for data reduction or feature importance determination prior to making classification attempts. This lab will attempt a variety of clustering methods who's outputs will then be used in an ensemble learning model based on the most successful classification algorithms from Lab 2, namely the decision tree based classifiers that performed best.\n",
    "### Sources:\n",
    "We make extensive use of the code provided in the three clustering Notebooks for the class and code from Sebastian Raschka book \"Python Machine Learning\".\n",
    "## Business Understanding\n",
    "We've outlined the business case for understanding these data in previous labs, but as a reminder, Expedia has provided logs of customer behavior captured during their online search sessions. The purpose of these data are to take customer behavior and use it to predict which hotel clusters the customers are likely to book. Hotel clusters are based on Expedia's algorithm which groups hotels based on price, customer ratings, and location. A hotel cluster example might be hotels located in a downtown location in the $250-300 per night price range with an average rating between 3-4 stars (out of 5). However since the data is from Kaggle all the clusters are masked by an id. We are unable to see the star rating or specific location of each hotel cluser. Expedia has provided a training data set which is a random sample of customer behavior and hotel cluster bookings from 2013-2014. The objective is to use data mining techniques to develop a predictive algorithm based on these data and then apply it to a test data set consisting of randomly sampled customer behavior from 2015. Since this is part of a Kaggle competition, only Expedia knows the true outcome of how 2015 customers booked by hotel cluster. The success of the predictive algorithm will be based on the rate of accurate classification of 2015 Expedia customers into the appropriate hotel cluster. A successful submission will include 5 hotel cluster recommendations for each line item of the test data set. Scoring is based on whether the correct cluster is presented in the 5 clusters as well as where it is ranked in the 5 recommendation. The test data set has two less variables than the training data set which includes is booked since all the test data is actual bookings and the hotel cluster chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "We've already discussed the prohibitive size of the original dataset and reduced its size accordingly. After reduction, the variable that measures distance between a user's target hotel and location from which they are conducting the search (orig_destination_distance) was missing in approximately 25% of the records. Retention of this feature was initially desirable because we had very few continuous variables, and as we later discovered in Lab 2 preprocessing it proved to be of high importance in all of our classification algorithms. We were able to successfully impute 99.6% of the missing values using the median distance between a user location and destination as derived from three features ('posa_continent', 'hotel_continent', 'hotel_country'). We dropped the remaining 397 records that could not be imputed leaving us with a final dataset with no NULL values.\n",
    "### Data Subset\n",
    "In Lab 2 we conducted extensive data preprocessing to determine which features were the most important in the context of each algorithm. This was done using a Sequential Backward Selection algorithm as well as each algorithm's built-in feature importance function. We found a smaller subset of 7 features that were important to at least one of the three primary algorithms being tested as well as a very small subset of 2 features that were unanimously important to all of the algorithms. In the context of clustering we are going to test on the full (22 features) and medium (7 features) sized datasets to determine which performs better to delineate clusters.\n",
    "### Data import and subset generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduced, imputed, file. This will represent the full dataset once we remove is_booking.\n",
    "# Data can be downloaded here: https://www.dropbox.com/s/2vcqmorh3n3cm21/train_booked_top5_imputed.csv?dl=0\n",
    "full_df = pd.read_csv('data/train_booked_top5_imputed.csv')\n",
    "full_df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that is_booking is all true so we'll remove since it's intuitively an unimportant feature.\n",
    "full_df.drop(['is_booking'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create reduced df based on most important features from our algorithms' important features function.\n",
    "# The feature importance threshold is 9% to be included:\n",
    "medium_df = full_df[['hotel_cluster','hotel_market','srch_destination_id','orig_destination_distance',\n",
    "                            'user_id','user_location_city','user_location_region','hotel_country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These features yielded the best performance in SBS and feature importance for random forest classifier:\n",
    "small_df = full_df[['hotel_cluster','hotel_market','srch_destination_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Clustering strategy\n",
    "We will begin with the medium dataframe since we have determine these features to be important and we can already because to see which clusters make sense. Hotel market, hotel country, and search destination id should be a logical grouping, as well as user location city and region with the user ID a potential addition as well. Origin destination disance being a stand alone continuous variable we will keep it separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "### Train and Adjust Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate and Compare\n",
    "\n",
    "### Visualize Results\n",
    "\n",
    "### Summarize the Ramifications\n",
    "\n",
    "## Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
